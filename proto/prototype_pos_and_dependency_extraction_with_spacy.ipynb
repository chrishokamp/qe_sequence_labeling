{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IDEA: we want to make a corpus of POS and dependency features which can be mapped to embeddings\n",
    "# IDEA: source/target POS and dependencies will be factors in a multilingual model \n",
    "\n",
    "# TODO: remember splitting based on tokenization\n",
    "# TODO: first annotate unsegmented data with Spacy, then split by subwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'They', 757862, u'-PRON-', 479, u'PRP', 93, u'PRON')\n",
      "(u'told', 971, u'tell', 489, u'VBD', 98, u'VERB')\n",
      "(u'us', 757862, u'-PRON-', 479, u'PRP', 93, u'PRON')\n",
      "(u'to', 504, u'to', 486, u'TO', 92, u'PART')\n",
      "(u'duck', 7797, u'duck', 474, u'NN', 90, u'NOUN')\n",
      "(u'.', 453, u'.', 453, u'.', 95, u'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'They told us to duck.')\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)\n",
    "\n",
    "# test removing tokenizer from pipeline -- doesn't work\n",
    "# nlp.pipeline = nlp.pipeline[1:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'I', u'I', u'nsubj', u'like')\n",
      "(u'green eggs', u'eggs', u'dobj', u'like')\n",
      "(u'ham', u'ham', u'conj', u'eggs')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I like green eggs and ham.')\n",
    "for np in doc.noun_chunks:\n",
    "    print(np.text, np.root.text, np.root.dep_, np.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_news = nlp(u'More than 30 civilians and Kurdish-led fighters have been killed in an attack by Islamic State militants near Syria\\'s north-eastern border with Iraq.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<spacy.tagger.Tagger at 0x7f8985807d20>,\n",
       " <spacy.pipeline.DependencyParser at 0x7f8985ae2050>,\n",
       " <spacy.matcher.Matcher at 0x7f8987eb1d70>,\n",
       " <spacy.pipeline.EntityRecognizer at 0x7f8987e0c788>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = test_news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'More ',\n",
       " u'than ',\n",
       " u'30 ',\n",
       " u'civilians ',\n",
       " u'and ',\n",
       " u'Kurdish',\n",
       " u'-',\n",
       " u'led ',\n",
       " u'fighters ',\n",
       " u'have ',\n",
       " u'been ',\n",
       " u'killed ',\n",
       " u'in ',\n",
       " u'an ',\n",
       " u'attack ',\n",
       " u'by ',\n",
       " u'Islamic ',\n",
       " u'State ',\n",
       " u'militants ',\n",
       " u'near ',\n",
       " u'Syria',\n",
       " u\"'s \",\n",
       " u'north',\n",
       " u'-',\n",
       " u'eastern ',\n",
       " u'border ',\n",
       " u'with ',\n",
       " u'Iraq',\n",
       " u'.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.text_with_ws for w in test_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'More '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = [(w.text, w.tag_, w.dep_, w.head.tag_, w.head.dep_) for w in test_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text, factors = zip(*[(factor_tup[0], factor_tup[1:]) for factor_tup in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text, factors = zip([(factor_tup[0], factor_tup[1:]) for factor_tup in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'JJR|amod|CD|nummod IN|quantmod|CD|nummod CD|nummod|NNS|nsubjpass NNS|nsubjpass|VBN|ROOT CC|cc|NNS|nsubjpass JJ|npadvmod|VBN|amod HYPH|punct|VBN|amod VBN|amod|NNS|conj NNS|conj|NNS|nsubjpass VBP|aux|VBN|ROOT VBN|auxpass|VBN|ROOT VBN|ROOT|VBN|ROOT IN|prep|VBN|ROOT DT|det|NN|pobj NN|pobj|IN|prep IN|prep|NN|pobj JJ|amod|NNS|pobj NN|compound|NNS|pobj NNS|pobj|IN|prep IN|prep|NNS|pobj NNP|poss|NN|pobj POS|case|NNP|poss NN|compound|JJ|amod HYPH|punct|JJ|amod JJ|amod|NN|pobj NN|pobj|IN|prep IN|prep|NN|pobj NNP|pobj|IN|prep .|punct|VBN|ROOT'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u' '.join([u'|'.join(f) for f in factors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'JJR', u'amod', u'CD', u'nummod'),\n",
       " (u'IN', u'quantmod', u'CD', u'nummod'),\n",
       " (u'CD', u'nummod', u'NNS', u'nsubjpass'),\n",
       " (u'NNS', u'nsubjpass', u'VBN', u'ROOT'),\n",
       " (u'CC', u'cc', u'NNS', u'nsubjpass'),\n",
       " (u'JJ', u'npadvmod', u'VBN', u'amod'),\n",
       " (u'HYPH', u'punct', u'VBN', u'amod'),\n",
       " (u'VBN', u'amod', u'NNS', u'conj'),\n",
       " (u'NNS', u'conj', u'NNS', u'nsubjpass'),\n",
       " (u'VBP', u'aux', u'VBN', u'ROOT'),\n",
       " (u'VBN', u'auxpass', u'VBN', u'ROOT'),\n",
       " (u'VBN', u'ROOT', u'VBN', u'ROOT'),\n",
       " (u'IN', u'prep', u'VBN', u'ROOT'),\n",
       " (u'DT', u'det', u'NN', u'pobj'),\n",
       " (u'NN', u'pobj', u'IN', u'prep'),\n",
       " (u'IN', u'prep', u'NN', u'pobj'),\n",
       " (u'JJ', u'amod', u'NNS', u'pobj'),\n",
       " (u'NN', u'compound', u'NNS', u'pobj'),\n",
       " (u'NNS', u'pobj', u'IN', u'prep'),\n",
       " (u'IN', u'prep', u'NNS', u'pobj'),\n",
       " (u'NNP', u'poss', u'NN', u'pobj'),\n",
       " (u'POS', u'case', u'NNP', u'poss'),\n",
       " (u'NN', u'compound', u'JJ', u'amod'),\n",
       " (u'HYPH', u'punct', u'JJ', u'amod'),\n",
       " (u'JJ', u'amod', u'NN', u'pobj'),\n",
       " (u'NN', u'pobj', u'IN', u'prep'),\n",
       " (u'IN', u'prep', u'NN', u'pobj'),\n",
       " (u'NNP', u'pobj', u'IN', u'prep'),\n",
       " (u'.', u'punct', u'VBN', u'ROOT'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'More', u'JJR', u'amod', u'CD', u'nummod', []),\n",
       " (u'than', u'IN', u'quantmod', u'CD', u'nummod', []),\n",
       " (u'30', u'CD', u'nummod', u'NNS', u'nsubjpass', []),\n",
       " (u'civilians', u'NNS', u'nsubjpass', u'VBN', u'ROOT', [fighters]),\n",
       " (u'and', u'CC', u'cc', u'NNS', u'nsubjpass', []),\n",
       " (u'Kurdish', u'JJ', u'npadvmod', u'VBN', u'amod', []),\n",
       " (u'-', u'HYPH', u'punct', u'VBN', u'amod', []),\n",
       " (u'led', u'VBN', u'amod', u'NNS', u'conj', []),\n",
       " (u'fighters', u'NNS', u'conj', u'NNS', u'nsubjpass', []),\n",
       " (u'have', u'VBP', u'aux', u'VBN', u'ROOT', []),\n",
       " (u'been', u'VBN', u'auxpass', u'VBN', u'ROOT', []),\n",
       " (u'killed', u'VBN', u'ROOT', u'VBN', u'ROOT', []),\n",
       " (u'in', u'IN', u'prep', u'VBN', u'ROOT', []),\n",
       " (u'an', u'DT', u'det', u'NN', u'pobj', []),\n",
       " (u'attack', u'NN', u'pobj', u'IN', u'prep', []),\n",
       " (u'by', u'IN', u'prep', u'NN', u'pobj', []),\n",
       " (u'Islamic', u'JJ', u'amod', u'NNS', u'pobj', []),\n",
       " (u'State', u'NN', u'compound', u'NNS', u'pobj', []),\n",
       " (u'militants', u'NNS', u'pobj', u'IN', u'prep', []),\n",
       " (u'near', u'IN', u'prep', u'NNS', u'pobj', []),\n",
       " (u'Syria', u'NNP', u'poss', u'NN', u'pobj', []),\n",
       " (u\"'s\", u'POS', u'case', u'NNP', u'poss', []),\n",
       " (u'north', u'NN', u'compound', u'JJ', u'amod', []),\n",
       " (u'-', u'HYPH', u'punct', u'JJ', u'amod', []),\n",
       " (u'eastern', u'JJ', u'amod', u'NN', u'pobj', []),\n",
       " (u'border', u'NN', u'pobj', u'IN', u'prep', []),\n",
       " (u'with', u'IN', u'prep', u'NN', u'pobj', []),\n",
       " (u'Iraq', u'NNP', u'pobj', u'IN', u'prep', []),\n",
       " (u'.', u'.', u'punct', u'VBN', u'ROOT', [])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flow\n",
    "# mkdir <data-root>/factored-corpus/\n",
    "# export CORPUS_OUTPUT_DIR=<data-root>/factored-corpus/\n",
    "# extract factored corpus into $CORPUS_OUTPUT_DIR\n",
    "# One file for each factor -- this is because we need to do subword for text only, not for tags\n",
    "# learn subword on train text data according to `subword-nmt` recommended best practices\n",
    "\n",
    "# learn 40000 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
